---
title: "surveyCV: Cross Validation Based on Survey Design"
author: "Cole Guerin, Thomas McMahon, Jerzy Wieczorek"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{intro}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include=FALSE}
library(surveyCV)

library(survey)
library(ISLR)
library(dplyr)
library(splines)
library(gridExtra)
library(grid)
library(ggplot2)
library(lattice)
```



# Approach

The purpose of the beginning of this document is to outline our findings when investigating the effects of accounting for complex survey designs on cross validation error estimates. We decided to look at differences in cross validation when using SRS, cluster, and stratification sampling designs. We decided that the difference in cross validation, based on these survey designs, would mainly come from how the folds for K-fold cross validation are generated. Here was our approach to this in our functions:

When generating folds, if an SRS model is used, observations will be assigned randomly to each fold. If a clustered model is used, observations within each cluster are kept together so at least one full cluster will be in each fold. When folds are generated with a stratified model, the strata are broken up so at least one observation from each strata are in each fold. 

We also decided that sampling design would come into play in our functions when the survey model object is created and when the mean squared error estimates are generated. In these instances in our functions, we just vary the survey design object indicating which sampling design was used.

We broke down our function as to be able to vary each of these uses (fold generation, model creation, MSE estimation) in order to determine which was influencing the differences observed. The results suggest fold generation appears to cause the greatest difference and the best examples can be seen below. The last part of this document will be some explicit examples of how to use the functions of our package.

# Key Findings

We simulated data that would be useful for spline modeling, and took clustered and stratified samples from this data set representative of clustered and stratified samples of survey data. We found that if a clustered sample was taken, not taking into consideration clustering during cross validation would lead to overconfidence in our results while for stratified samples, the opposite would occur. Below we have included the plots that best support our findings while the rest of the document contains further tests we ran on various sample sizes further supporting the importance of taking into consideration survey design when performing cross validation.

Here is the simulated data that we were working with:

```{r, fig.width = 8, fig.height = 6}
set.seed(47)
x1 = runif(1:500, min = 26, max = 38)
y1 = (x1-29)^3 - 13*(x1-29)^2 + 0*(x1-29) + 900

set.seed(47)
x2 = runif(1:500, min = 38, max = 50)
y2 = (x2-36)^3 - 10*(x2-36)^2 + 2*(x2-36) + 600

set.seed(47)
z1 = jitter(y1, 15000)
z1 = jitter(y1, 15000)
z2 = jitter(y2, 15000)

ds1 <- data.frame(Response = z1, Predictor = x1)
ds2 <- data.frame(Response = z2, Predictor = x2)

ds <- rbind(ds1, ds2)

b <- data.frame(ID = c(1:1000))
spline.df2 <- cbind(b, ds)
spline.df2 <- spline.df2 %>%
  arrange(Predictor) %>%
  mutate(Stratum = row_number(),
         Cluster = row_number())  
spline.df2$Stratum <- cut(spline.df2$Stratum,5, 1:5)
spline.df2$Cluster <- cut(spline.df2$Cluster,100, 1:100) 
spline.df2 <- spline.df2 %>%
  arrange(ID) %>%
  select(ID, Response, Predictor, Cluster, Stratum)

n=100
set.seed(32)
srs.df <- sample_n(spline.df2, n)

set.seed(32)
s <- stratsample(spline.df2$Stratum, c("1" = n/5, "2" = n/5, "3" = n/5, "4" = n/5, "5" = n/5))
strat.df <- spline.df2[s,]

set.seed(32)
c <- unique(spline.df2[["Cluster"]]) 
clus.df <- spline.df2[spline.df2[["Cluster"]] %in% sample(c, n/10),]

a <- ggplot(mapping = aes(x = spline.df2$Predictor, y = spline.df2$Response)) + geom_point(shape = 1) +
  labs(title = "Simulated Data and Samples", x = "Predictor", y = "Response") + ylim(100,1650) +
  geom_point(mapping = aes(x = srs.df$Predictor, y = srs.df$Response), color = "darkgreen", shape = 8) +
  geom_point(mapping = aes(x = strat.df$Predictor, y = strat.df$Response), color = "darkred", shape = 3) +
  geom_point(mapping = aes(x = clus.df$Predictor, y = clus.df$Response), color = "steelblue", shape = 4)
b <- ggplot(mapping = aes(x = srs.df$Predictor, y = srs.df$Response)) + geom_point(shape = 8, color = "darkgreen") +
  labs(title = "Simple Random Sample", x = "Predictor", y = "Response") + ylim(100,1650)
c <- ggplot(mapping = aes(x = strat.df$Predictor, y = strat.df$Response)) + geom_point(shape = 3, color = "darkred") +
  labs(title = "Stratification Sample", x = "Predictor", y = "Response") + ylim(100,1650)
d <- ggplot(mapping = aes(x = clus.df$Predictor, y = clus.df$Response)) + geom_point(shape = 4, color = "steelblue") +
  labs(title = "Cluster Sample", x = "Predictor", y = "Response") + ylim(100,1650)
grid.arrange(a, b, c, d, ncol = 2, top = "Simulated Data And How It Was Sampled")
```

First we needed to prove that sampling method actually produced a difference in the estimates our function was giving. Below we see plots of the estimates produced from either a simple random sample or clustered sample of size 100 using either a simple random sample or clustered method of fold generation (every test in this part of the  document uses 5 folds). The cluster sampling in our simulations exaggerated the high design effects typically seen in real cluster samples. The below graphs show a distinct difference when using cluster fold generation with a clustered sample, suggesting that taking into account sampling design when doing survey based cross validation may be important. 

```{r, fig.width = 8, fig.height = 6, cache = TRUE}
sim.clusvsrs.sd.n100
```

Since we determined that taking into consideration sampling design effected our estimates, we went on to determine what part of our function was causing this difference. We take into consideration survey design in 3 sections of our function. The first is when folds are being generated, the second is when a model is being fit to the folds, and the third is when mean squared errors of a model are being estimated.


```{r, fig.width = 8, fig.height = 8, cache = TRUE}
sim.clusvsrs.bp.n100
```


Here we see the results of 100 runs of our cross validation function on clustered samples of our simulated data. In the Title above each plot, S represents Sampling method, F represents the method used for fold generation, M represents the survey design assumed by the spline model fits, and MSE represents the survey design used to average the mean squared errors. The sample size used for the above plots was 100 as the effects of including sampling design are most prominent at lower sampling sizes. As we can see, when we ignored the clustered sampling design when generating folds and instead used a simple random sample (SRS) fold generation method (rows 1 and 2) we see much lower MSE estimates, and the MSEs were typically minimized by a higher degrees of freedom (DF) spline model. When we did take into consideration the clustered sampling design when generating folds, we got much higher MSEs overall and a lower DF spline model being chosen. These findings are representative of the lower effective sampling size caused by clustered sampling and suggest that using an SRS model during fold generation would lead to overconfidence and overfitting. Meanwhile, taking the clustering into account appropriately leads to simpler models being chosen.


```{r, fig.width = 8, fig.height = 8, cache = TRUE}
sim.stratvsrs.bp.n100
```


Next we will discuss stratified samples. Above we have a similar set of plots, with the sample size equal to 100, but instead using a stratified sample instead of a clustered one. The effects are harder to see here but, when using a SRS design to generate folds (rows 1 and 2),  the cross validation function has higher MSEs and may even choose a lower degree of freedom. When stratification is taken into consideration during fold generation, the MSEs appear lower, and models with greater DFs may be chosen. This is presumed to be caused by the overall increase in effective sampling size obtained when taking stratified samples which allows appropriately for higher confidence in more complex models.

# Examples Using surveyCV Functions

Notes: 
- The left column of the output gives the MSE estimates for the models, with the rows corresponding to the order that the models were input into the function.
- All of these examples use the Auto data set from the ISLR package.
- See help documentation for each function if you have questions about the parameters and possible input.

## cv.srs.lm()

MSEs generated for a SRS test of a first and second degree polynomial fit predicting mpg from horsepower in the Auto data set.

```{r}
data("Auto")
cv.srs.lm(Auto, c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)"), nfolds = 10, N = 400)
```

## cv.cluster.lm()

MSEs generated for a clustered test of a first and second degree polynomial fit predicting mpg from horsepower in the Auto data set, clustering using the "year" variable.

```{r}
data("Auto")
cv.cluster.lm(Auto, c("mpg~poly(horsepower,1,raw=TRUE)", "mpg~poly(horsepower,2,raw=TRUE)"), nfolds = 10, "year", 400)
```

## cv.strat.lm()

MSEs generated for a stratified test of a first and second degree polynomial fit predicting mpg from horsepower in the Auto data set, Stratified on the "year" variable.

```{r}
data("Auto")
cv.strat.lm(Auto, c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)"), nfolds = 10, "year", 400)
```

## cv.svy.glm()

MSEs generated for different tests of first and second degree polynomial fits predicting mpg from horsepower in the Auto data set. Clustering and Stratification were done along the was done along the "year" variable.

```{r}
data("Auto")
cv.svy.glm(Auto, c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)"), nfolds = 10, N = 400)
cv.svy.glm(Auto, c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)"), nfolds = 10, N = 400, clusterID = "year", sample_type = "Cluster")
cv.svy.glm(Auto, c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)"), nfolds = 10, N = 400, strataID = "year", sample_type = "Strat")
```

Using either a survey design object or survey glm to generate MSEs.

```{r}
data("Auto")
auto.srs.svy <- svydesign(ids = ~0,
                          strata = NULL,
                          fpc = rep(1000, 392),
                          data = Auto)
srs.model <- svyglm(mpg~horsepower+I(horsepower^2)+I(horsepower^3), design = auto.srs.svy)
auto.clus.svy <- svydesign(ids = ~year,
                           strata = NULL,
                           fpc = rep(1000, 392),
                           data = Auto)
clus.model <- svyglm(mpg~horsepower+I(horsepower^2)+I(horsepower^3), design = auto.clus.svy)
auto.strat.svy <- svydesign(ids = ~0,
                            strata = ~year,
                            fpc = rep(1000, 392),
                            data = Auto)
strat.model <- svyglm(mpg~horsepower+I(horsepower^2)+I(horsepower^3), design = auto.strat.svy)
cv.svy.glm(formulae = c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)",
                        "mpg~poly(horsepower,3, raw = TRUE)"), design_object = auto.srs.svy, nfolds = 10, N = 1000)
cv.svy.glm(formulae = c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)",
                        "mpg~poly(horsepower,3, raw = TRUE)"), design_object = auto.clus.svy, nfolds = 10, N = 1000)
cv.svy.glm(formulae = c("mpg~poly(horsepower,1, raw = TRUE)", "mpg~poly(horsepower,2, raw = TRUE)",
                        "mpg~poly(horsepower,3, raw = TRUE)"), design_object = auto.strat.svy, nfolds = 10, N = 1000)
cv.svy.glm(glm = srs.model, nfolds = 10, N = 1000)
cv.svy.glm(glm = clus.model, nfolds = 10, N = 1000)
cv.svy.glm(glm = strat.model, nfolds = 10, N = 1000)
```
